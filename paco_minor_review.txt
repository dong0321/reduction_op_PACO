Manuscript Number: PARCO-D-21-00023R1  

Using Long Vector Extensions for MPI Reductions  

Dear Mr zhong,    

Thank you for submitting your manuscript to Parallel Computing.  

I have completed my evaluation of your manuscript. The reviewers recommend reconsideration of your manuscript following minor revision and modification. I invite you to resubmit your manuscript after addressing the comments below. Please resubmit your revised manuscript by Sep 26, 2021.

When revising your manuscript, please consider all issues mentioned in the reviewers' comments carefully: please outline every change made in response to their comments and provide suitable rebuttals for any comments not addressed. Please note that your revised submission may need to be re-reviewed.    

To submit your revised manuscript, please log in as an author at https://www.editorialmanager.com/parco/, and navigate to the "Submissions Needing Revision" folder under the Author Main Menu.    

Parallel Computing values your contribution and I look forward to receiving your revised manuscript.￼  

Kind regards,   

Umit Catalyurek  

Editor-in-Chief  

Parallel Computing  

Editor and Reviewer comments:    



Reviewer #1: Thank you for addressing the comments from my review. I am still concerned about this revised statement in section 7.

"It can be observed
that the performance benefit increases with more processes/nodes, because of the strong-scaling nature of the application,
which translate into the more MPI processes participate in the
reduction operation, the larger the data is. Thus, the fact that
each one of them is simultaneously using our AVXs optimized
Open MPI operations drives up the overall application performance."

You state that the application is "strong-scaling". To me strong-scaling means a
fixed problem size no matter how many processes participate in the reduction operation.
That is, the reduction work per-process goes *down* as you scale up, so it is counterintuitive
to me that with fewer processes the benefit is less, as the computation should make up a
larger part of the total execution.
Does the chart in figure 13 include both communication and computation in the measurement?
Could it be broken down to show how each contributed to the total?



Reviewer #2: Thank you for the revision to your paper.

In general, my original review stands: this is excellent research work
and should be published.

Revision 1 is a notable improvement compared to the original
manuscript.

As a minor -- but important -- point, please proof read the paper
again.  For example, I see at least one typo in the abstract, and at
least 5 gramatical errors on page 2.  Just to show that I'm not making
up these numbers, here's a list:

Abstract:

"... is not only efficient but also generalizable to many vector
archiecture and efficient."

The word "efficient" is used twice and "architecture" should be
plural.

Page 2:

1st col 1st para: ...packing either double-prevision, [or] sixteen
single-precision floating-point numbers"
--> insert "or" to match the 2nd clause

1st col 2nd para: "... a vector extension for [the] AArch64 execution
mode..."
--> insert "the"

2nd col 1st para: "The MPI standard provides [a] fully fledged..."
--> replace "an" --> "a"

2nd col 2nd para: "by provid[ing] support for..."
---> replace "provide" --> "providing"

2nd col 4th para: "... reduction operation[s] with a large..."
--> "operations" needs to be plural

Thank you for removing most flowery descriptions and adjectives.  I
think one was missed: page 2, 1st col: "The product offers a high
degree of compiler support by including [a unique level of
richness]..."

Bullets 1-3 in page 2 should use parallel construction.  I.e., bullets
1 and 2 start with a thing (a "rich addressing mode" and a "set of
horizontal reduction operations") -- these are clearly features, as
described in the paragraph above the bullets.  The 3rd bullet starts
with a verb instead of a noun, making the 3rd bullet seem out of place
with the other 2.  I suggest something like "the capability of
vectorization of loops..."

In a few places in the paper, the name "MPI_Reduce_Local" is used.
The "l" in local should be lower case: "MPI_Reduce_local".

Additionally, througout the paper, sometimes the name "MPI_Allreduce"
is used, but sometimes "AllReduce" is used.  I could not quite figure
out if there was an actual distinction between the two, or whether
"AllReduce" was meant to be an abrreviation for "MPI_Allreduce".
Please either explain the distinction or be consistent in using
"MPI_Allreduce".

Last paragraph of section 2.2: did you mean the x86 ISA, or the x86-64
ISA?  (or both?)

On page 5, in the description of the intrinsic functions:

_mm512_<op>_epi32: it says that the results are stored in a
destination.  Is "b" an inout, and is therefore also the destination?
That would be worth clarifying.

In svld1 and svst1: the 1st parameter is "Pg" -- but it has no type,
unlike all the other parameters in all the other intrinsics.

Figure 3:
- The directory name mca/op/avx is listed in the orange box, but is
  not explained.  Listing the directory name doesn't seem to add much
  value to the image.
- The caption says "The organge boxes..." but there's only 1 orange
  box.
- PRRTE, OPAL, RM, and OS are listed in other boxes in the diagram,
  but are not explained.  I realize that these are not the focus of
  the paper, but if you're going to show an architectural diagram of
  Open MPI, you should at least provide one or two words saying what
  they are.  E.g., say "PRRTE (run-time support)" / "OPAL (portability
  layer)" ... etc.

I still do not understand Figure 4 at all.  From a comment in my
original review:

- I do not understand figure 4: it appears to be some kind of limited
 flow chart, but not all possible paths are shown.  For example,
 there are several paths/arrows missing (e.g., *some* "no" / not
 supported paths are shown, but not all of them.  Why?).  Also, what
 happens when multiple AVX/SSE flags are supported -- do *all* three
 of the leaf boxes get used?  Please clarify this figure, e.g., by
 using commonly-accepted flow chart notation.

Also, the caption on Figure 4 is not accurate: I don't think the flow
chart shows any "integration" at all.

The paragraph before algorithm 1 is improved, but is still a little
confusing.  I do not fully understand the differences between options
(2) and (3) -- both of them appear to be using intrinsic functions.
There appears to be no text describing option (3), so I'm not clear
on how it is different than (2).

The paragraph closes by saying "we chose to use option 2 to maximize
performance."  I think it might also be worth mentioning that not only
are you optimizing performance, you're doing it the
diffcult-but-best-results way because this is hidden in the MPI
middleware, and therefore (many) applications benefit from it.  Hence,
the extra difficulty and corresponding performance gain is much more
worth it (than, for example, doing the difficult way for a single
application).

It doesn't look like Algorithm 1 changed much (at all?).  I still have
many of the same questions I put in my original review:

 - Why isn't the datatype a parameter of ReductionOp?  I know this
   is pseudo-code, but that's a necessary input to the algorithm,
   right?

 - "sizeof_type -> Number of bytes of the type in the inbuf /
   inout_buf".  How exactly is this value relevant?  I do not
   understand how the ratio of bytes in the inbuf to the inout buf
   would be useful here.

   --> Are you trying to say "and" instead of "divide"?  That would
       make much more sense.  I.e., say "and" instead of "/" --
       because reading "/" in a mathematical / pseudocode context, I
       read it to be "divide", not "and".

 - Line 2: what does the notation "vector_length(512)" represent?  Is
   that supposed to be a constant 512, or is there a multiplier or
   divisor involved?  The value to the right of the "/" operator
   appears to be a number of bits (assuming sizeof_type is actually a
   number of bytes, not a ratio as denoted in its description).
   Does that mean "vector_length(512)" is a constant 512?  If so, I
   do not understand what the "vector_length(...)" notation is
   supposed to mean.  Should vector_length be a simple variable, and
   perhaps an input parameter to RedunctionOp?

 - Line 4: I do not understand the loop "for k <- types_per_step to
   count".  Example: I call this ReductionOp with 8 billion MPI_INTs,
   where sizeof(int) (in C) is 4 bytes.  This would therefore
   represent 32 gigabytes of data.

   Assuming that:
   - sizeof_type = 4 (i.e., C sizeof(int) == 4)
   - count = 2^34
   - types_per_step = 512 / (8 * 4) = 16

   This means that K will start at 16 and increment by 1 until it
   gets to 2^34 (i.e., 16, 17, 18, 19, ..., 2^34).  Shouldn't the
   loop be from 0 to count, stepping by types_per_step (0, 16, 32,
   64, ..., 2^34-16)?

 - Additionally, doesn't some kind of offset need to be denoted in
   lines 5-8, and 13-16, and 21-24, and 29?

I have essentially the same questions about Algorithm 2.

Additionally, Alg 2 seems to use "#op_sign#" notation where Alg 1 uses
"<op>".  Please be consistent.

Figure 5 and its corresponding text were not changed from the original
manuscript.  I still have all the same questions that I put in my
original review.  For example: is each column supposed to represent a
distinct "a <op> b" operation?  If so, why is there only a dotted line
between the one of the red columns and one of the yellow columns?
Shouldn't there be a dotted line between all the columns?  I'm still
quite confused as to exactly what Figure 5 adds to the paper.

Page 8, last full paragraph in column 1: there's an explanation for
the symbols (stars, circles, arrows) in the graphs: "represent flier
data that extend beyond the whiskers."  I thought that that phrase was
a colloquialism and had to look up what it meant.  My opinion is that
you might want to use a more common phrase (or perhaps even the word
"outlier" instead of "flier"), but perhaps I, myself, am the outlier
here for not ever having heard that phrase before...

Figure 12 and its supporting text: you probably don't need to use and
explain the "op ^avx" Open MPI command line syntax (which is
incorrectly denoted in the text, anyway).  Just say that no AVX
operations were used.

There's still a lot of bibtex entries are that incorrectly lower
cased.  Indeed, there's even disagreements between entries -- some
entries say "ARM" where others say "arm", for example.  Please review.

Reference 20 can now be updated to say MPI-4.0 -- it's not a draft any
more.

The URL in reference 44 looks wrong -- perhaps a latex-ism?

-----

Most of my comments are fairly minor.  But I think that at a minimum,
the questions about Algorithms 1 and 2 need to be answered -- I do not
think that the algorithm is correctly represented (or at least is not
explained so that the author and the reader can share the same
understanding of the algorithms).

Please note that I offer all of the above as constructive criticism in
an attempt to help convey the excellent work and gained wisdom from
this research and contribute to the state of the art.

I sign all of my reviews; please feel free to contact me about any
part of this review.

Jeffrey M. Squyres, Ph.D.
Cisco Systems, Inc.
jsquyres@cisco.com



Reviewer #3: Authors have addressed my comments. The manuscript is ready for publication.
